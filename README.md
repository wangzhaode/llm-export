# llm-export

llm-exportæ˜¯ä¸€ä¸ªllmæ¨¡å‹å¯¼å‡ºå·¥å…·ï¼Œèƒ½å¤Ÿå°†llmæ¨¡å‹å¯¼å‡ºåˆ°onnxæ¨¡å‹ã€‚

- ğŸš€ å‡å®Œæˆ`onnxruntime`æ­£ç¡®æ€§æµ‹è¯•
- ğŸš€ ä¼˜åŒ–åŸå§‹ä»£ç ï¼Œæ”¯æŒåŠ¨æ€å½¢çŠ¶
- ğŸš€ ä¼˜åŒ–åŸå§‹ä»£ç ï¼Œå‡å°‘å¸¸é‡éƒ¨åˆ†


## æ¨¡å‹æ”¯æŒä¸ä¸‹è½½
- [![Download][download-chatglm-6b-onnx]][release-chatglm-6b-onnx]
- [![Download][download-chatglm2-6b-onnx]][release-chatglm2-6b-onnx]
- [![Download][download-chatglm3-6b-onnx]][release-chatglm3-6b-onnx]
- [![Download][download-codegeex2-6b-onnx]][release-codegeex2-6b-onnx]
- [![Download][download-qwen-7b-chat-onnx]][release-qwen-7b-chat-onnx]
- [![Download][download-baichuan2-7b-chat-onnx]][release-baichuan2-7b-chat-onnx]
- [![Download][download-llama2-7b-chat-onnx]][release-llama2-7b-chat-onnx]
- [![Download][download-qwen-1.8b-chat-onnx]][release-qwen-1.8b-chat-onnx]

[download-chatglm-6b-onnx]: https://img.shields.io/github/downloads/wangzhaode/llm-export/chatglm-6b-onnx/total
[download-chatglm2-6b-onnx]: https://img.shields.io/github/downloads/wangzhaode/llm-export/chatglm2-6b-onnx/total
[download-chatglm3-6b-onnx]: https://img.shields.io/github/downloads/wangzhaode/llm-export/chatglm3-6b-onnx/total
[download-codegeex2-6b-onnx]: https://img.shields.io/github/downloads/wangzhaode/llm-export/codegeex2-6b-onnx/total
[download-qwen-7b-chat-onnx]: https://img.shields.io/github/downloads/wangzhaode/llm-export/qwen-7b-chat-onnx/total
[download-baichuan2-7b-chat-onnx]: https://img.shields.io/github/downloads/wangzhaode/llm-export/baichuan2-7b-chat-onnx/total
[download-llama2-7b-chat-onnx]: https://img.shields.io/github/downloads/wangzhaode/llm-export/llama2-7b-chat-onnx/total
[download-qwen-1.8b-chat-onnx]: https://img.shields.io/github/downloads/wangzhaode/llm-export/qwen-1.8b-onnx/total
[release-chatglm-6b-onnx]: https://github.com/wangzhaode/llm-export/releases/tag/chatglm-6b-onnx
[release-chatglm2-6b-onnx]: https://github.com/wangzhaode/llm-export/releases/tag/chatglm2-6b-onnx
[release-chatglm3-6b-onnx]: https://github.com/wangzhaode/llm-export/releases/tag/chatglm3-6b-onnx
[release-codegeex2-6b-onnx]: https://github.com/wangzhaode/llm-export/releases/tag/codegeex2-6b-onnx
[release-qwen-7b-chat-onnx]: https://github.com/wangzhaode/llm-export/releases/tag/qwen-7b-chat-onnx
[release-baichuan2-7b-chat-onnx]: https://github.com/wangzhaode/llm-export/releases/tag/baichuan2-7b-chat-onnx
[release-llama2-7b-chat-onnx]: https://github.com/wangzhaode/llm-export/releases/tag/llama2-7b-chat-onnx
[release-qwen-1.8b-chat-onnx]: https://github.com/wangzhaode/llm-export/releases/tag/qwen-1.8b-onnx

## ç”¨æ³•
1. å°†è¯¥é¡¹ç›®cloneåˆ°æœ¬åœ°
```sh
git clnoe git@github.com:wangzhaode/llm-export.git
```
2. å°†éœ€è¦å¯¼å‡ºçš„LLMé¡¹ç›®cloneåˆ°æœ¬åœ°ï¼Œå¦‚ï¼šchatglm2-6b
```sh
git clone https://huggingface.co/THUDM/chatglm2-6b
# å¦‚æœhuggingfaceä¸‹è½½æ…¢å¯ä»¥ä½¿ç”¨modelscope
git clone https://modelscope.cn/ZhipuAI/chatglm2-6b.git
```
3. æ‰§è¡ŒLLMExporterå¯¼å‡ºæ¨¡å‹
```sh
cd mnn-llm
# å°†chatglm2-6båˆ†ä¸ºembedding, blocks, lmåˆ†åˆ«å¯¼å‡ºä¸ºonnxå¹¶è½¬æ¢ä¸ºmnn, å¹¶å¯¼å‡ºtokenizer.txt
python llm_export.py \
        --path ../chatglm2-6b \
        --export_split \
        --export_token \
        --export_mnn \
        --onnx_path ./chatglm2-6b-onnx \
        --mnn_path  ./chatglm2-6b-mnn 
```

## åŠŸèƒ½
- æ”¯æŒå°†æ¨¡å‹å®Œæ•´å¯¼å‡ºä¸ºä¸€ä¸ªonnxæ¨¡å‹ï¼Œä½¿ç”¨`--export`
- æ”¯æŒå°†æ¨¡å‹åˆ†æ®µå¯¼å‡ºä¸ºå¤šä¸ªæ¨¡å‹ï¼Œä½¿ç”¨`--export_split`
- æ”¯æŒå¯¼å‡ºæ¨¡å‹çš„è¯è¡¨åˆ°ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ªtokenï¼›å…¶ä¸­tokenä½¿ç”¨base64ç¼–ç ï¼›ä½¿ç”¨`--export_verbose`
- æ”¯æŒå¯¼å‡ºæ¨¡å‹çš„Embeddingå±‚ä¸ºä¸€ä¸ªonnxæ¨¡å‹ï¼Œä½¿ç”¨`--export_embed`ï¼ŒåŒæ—¶æ”¯æŒbf16æ ¼å¼ï¼Œä½¿ç”¨`--embed_bf16`
- æ”¯æŒåˆ†å±‚å¯¼å‡ºæ¨¡å‹çš„blockï¼Œä½¿ç”¨`--export_blocks`å¯¼å‡ºå…¨éƒ¨å±‚ï¼›ä½¿ç”¨`--export_block $id`å¯¼å‡ºæŒ‡å®šå±‚
- æ”¯æŒå¯¼å‡ºæ¨¡å‹çš„lm_headå±‚ä¸ºä¸€ä¸ªonnxæ¨¡å‹ï¼Œä½¿ç”¨`--export_lm`
- æ”¯æŒå¯¹æ¨¡å‹è¿›è¡Œå¯¹è¯æµ‹è¯•ï¼Œä½¿ç”¨`--test $query`ä¼šè¿”å›llmçš„å›å¤å†…å®¹
- æ”¯æŒåœ¨å¯¼å‡ºonnxæ¨¡å‹åä½¿ç”¨onnxruntimeå¯¹ç»“æœä¸€è‡´æ€§è¿›è¡Œæ ¡éªŒï¼Œä½¿ç”¨`--export_test`
- æ”¯æŒå°†tokenizerå¯¼å‡ºä¸ºæ–‡æœ¬æ–‡ä»¶ï¼Œä½¿ç”¨`--export_token`
- æ”¯æŒå°†å¯¼å‡ºçš„onnxæ¨¡å‹è½¬æ¢ä¸ºmnnæ¨¡å‹ï¼Œé»˜è®¤è½¬æ¢ä¸ºéå¯¹ç§°4bité‡åŒ–ï¼Œä½¿ç”¨`--export_mnn`
- æŒ‡å®šå¯¼å‡ºè·¯å¾„ä½¿ç”¨`--onnx_path`å’Œ`--mnn_path`

## å‚æ•°
```
usage: llm_export.py [-h] --path PATH [--type {chatglm-6b,chatglm2-6b,chatglm3-6b,codegeex2-6b,Qwen-7B-Chat,Qwen-1_8B-Chat,Baichuan2-7B-Chat,Llama-2-7b-chat-ms}]
                     [--onnx_path ONNX_PATH] [--mnn_path MNN_PATH] [--export_mnn] [--export_verbose] [--export_test] [--test TEST] [--export] [--export_split] [--export_token]
                     [--export_embed] [--export_lm] [--export_block EXPORT_BLOCK] [--export_blocks] [--embed_bf16]

LLMExporter

optional arguments:
  -h, --help            show this help message and exit
  --path PATH           path(`str` or `os.PathLike`):
                        Can be either:
                        	- A string, the *model id* of a pretrained model like `THUDM/chatglm-6b`. [TODO]
                        	- A path to a *directory* clone from repo like `../chatglm-6b`.
  --type {chatglm-6b,chatglm2-6b,chatglm3-6b,codegeex2-6b,Qwen-7B-Chat,Qwen-1_8B-Chat,Baichuan2-7B-Chat,Llama-2-7b-chat-ms}
                        type(`str`, *optional*):
                        	The pretrain llm model type.
  --onnx_path ONNX_PATH
                        export onnx model path, defaut is `./onnx`.
  --mnn_path MNN_PATH   export mnn model path, defaut is `./mnn`.
  --export_mnn          Whether or not to export mnn model after onnx.
  --export_verbose      Whether or not to export onnx with verbose.
  --export_test         Whether or not to export onnx with test using onnxruntime.
  --test TEST           test model inference with query `TEST`.
  --export              export model to an `onnx` model.
  --export_split        export model split to some `onnx` models:
                        	- embedding model.
                        	- block models.
                        	- lm_head model.
  --export_token        export llm tokenizer to a txt file.
  --export_embed        export llm embedding to an `onnx` model.
  --export_lm           export llm lm_head to an `onnx` model.
  --export_block EXPORT_BLOCK
                        export llm block [id] to an `onnx` model.
  --export_blocks       export llm all blocks to `onnx` models.
  --embed_bf16          using `bfloat16` replace `float32` in embedding.
```
