# llm-export

[English](./README_en.md)

llm-exportæ˜¯ä¸€ä¸ªllmæ¨¡å‹å¯¼å‡ºå·¥å…·ï¼Œèƒ½å¤Ÿå°†llmæ¨¡å‹å¯¼å‡ºä¸ºonnxå’Œmnnæ¨¡å‹ã€‚

- ğŸš€ ä¼˜åŒ–åŸå§‹ä»£ç ï¼Œæ”¯æŒåŠ¨æ€å½¢çŠ¶
- ğŸš€ ä¼˜åŒ–åŸå§‹ä»£ç ï¼Œå‡å°‘å¸¸é‡éƒ¨åˆ†
- ğŸš€ ä½¿ç”¨[OnnxSlim](https://github.com/inisis/OnnxSlim)ä¼˜åŒ–onnxæ¨¡å‹ï¼Œæ€§èƒ½æå‡çº¦5%; by [@inisis](https://github.com/inisis)
- ğŸš€ æ”¯æŒå°†loraæƒé‡å¯¼å‡ºä¸ºonnxå’Œmnn
- ğŸš€ Onnxæ¨ç†ä»£ç [OnnxLLM](https://github.com/inisis/OnnxLLM)

## ç”¨æ³•
1. å°†è¯¥é¡¹ç›®cloneåˆ°æœ¬åœ°
```sh
git clone git@github.com:wangzhaode/llm-export.git
```
2. å°†éœ€è¦å¯¼å‡ºçš„LLMé¡¹ç›®cloneåˆ°æœ¬åœ°ï¼Œå¦‚ï¼šchatglm2-6b
```sh
git clone https://huggingface.co/THUDM/chatglm2-6b
# å¦‚æœhuggingfaceä¸‹è½½æ…¢å¯ä»¥ä½¿ç”¨modelscope
git clone https://modelscope.cn/ZhipuAI/chatglm2-6b.git
```
3. å¯¼å‡ºæ¨¡å‹
```sh
cd mnn-llm
# å°†chatglm2-6bå¯¼å‡ºä¸ºonnxæ¨¡å‹
python llm_export.py --path ../chatglm2-6b --export onnx
# å°†chatglm2-6bå¯¼å‡ºä¸ºmnnæ¨¡å‹, é‡åŒ–å‚æ•°ä¸º4bit, blokc-wise = 128
python llm_export.py --path ../chatglm2-6b --export mnn --quant_bit 4 --quant_block 128
```

## åŠŸèƒ½
- æ”¯æŒå°†æ¨¡å‹ä¸ºonnxæˆ–mnnæ¨¡å‹ï¼Œä½¿ç”¨`--export onnx`æˆ–`--export mnn`
- æ”¯æŒå¯¹æ¨¡å‹è¿›è¡Œå¯¹è¯æµ‹è¯•ï¼Œä½¿ç”¨`--test $query`ä¼šè¿”å›llmçš„å›å¤å†…å®¹
- é»˜è®¤ä¼šä½¿ç”¨onnx-slimå¯¹onnxæ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œè·³è¿‡è¯¥æ­¥éª¤ä½¿ç”¨`--skip_slim`
- æ”¯æŒåˆå¹¶loraæƒé‡åå¯¼å‡ºï¼ŒæŒ‡å®šloraæƒé‡çš„ç›®å½•ä½¿ç”¨`--lora_path`

## å‚æ•°
```
usage: llm_export.py [-h] --path PATH [--type TYPE] [--lora_path LORA_PATH] [--dst_path DST_PATH] [--test TEST] [--export EXPORT] [--skip_slim] [--quant_bit QUANT_BIT] [--quant_block QUANT_BLOCK]

llm_exporter

optional arguments:
  -h, --help            show this help message and exit
  --path PATH           path(`str` or `os.PathLike`):
                        Can be either:
                        	- A string, the *model id* of a pretrained model like `THUDM/chatglm-6b`. [TODO]
                        	- A path to a *directory* clone from repo like `../chatglm-6b`.
  --type TYPE           type(`str`, *optional*):
                        	The pretrain llm model type.
  --lora_path LORA_PATH
                        lora path, defaut is `None` mean not apply lora.
  --dst_path DST_PATH   export onnx/mnn model to path, defaut is `./model`.
  --test TEST           test model inference with query `TEST`.
  --export EXPORT       export model to an onnx/mnn model.
  --skip_slim           Whether or not to skip onnx-slim.
  --quant_bit QUANT_BIT
                        mnn quant bit, 4 or 8, default is 4.
  --quant_block QUANT_BLOCK
                        mnn quant block, default is 0 mean channle-wise.
```
